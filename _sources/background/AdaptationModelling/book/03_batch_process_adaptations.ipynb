{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptation of CI - Calculate adaptations\n",
    "\n",
    "In this notebook, the goal is to calculate the risk of river flooding under a baseline scenario where no adaptation has happened, and compare it with what would the risk be if different adaptation levels were considered. The measures tested in this example range from purely from reducing the hazard (by building floodwall), asset-based adaptatio, network and system:\n",
    "\n",
    "- Level 1 - **Hazard level adaptation**: Bbuild a floodwall\n",
    "\n",
    "- Level 2 - **Asset level adaptation**: Elevate railway as viaduct\n",
    "\n",
    "- Level 3 - **Network level adaptation**: Create new bridges between assets\n",
    "\n",
    "- Level 4 - **System level adaptation**: Reduce traffic and freight demand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDE CODE\n",
    "import os\n",
    "# os.chdir('path/to/book/directory')\n",
    "os.chdir('..')\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd()))\n",
    "from src.ci_adapt_utilities import *\n",
    "\n",
    "data_path = Path(os.getcwd() + '/data')\n",
    "interim_data_path = data_path / 'interim' / 'collected_flood_runs'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load default configuration and model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file=Path(os.getcwd() + '/config_ci_adapt_test.ini')\n",
    "hazard_type, infra_type, country_code, country_name, hazard_data_subfolders, asset_data, vulnerability_data = load_config(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load asset data and adaptation cost data\n",
    "\n",
    "This step includes also loading the results of the Risk Analysis under the baseline scenario, where no adaptation options are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675 assets loaded.\n",
      "Loaded data from baseline impact assessment\n",
      "Creating virtual graph...\n",
      "Success: only int type values\n"
     ]
    }
   ],
   "source": [
    "assets, geom_dict, _, return_period_dict, adaptation_unit_costs, rp_spec_priority, average_road_cost_per_ton_km, average_train_cost_per_ton_km, average_train_load_tons = startup_ci_adapt(data_path, config_file)\n",
    "\n",
    "# Load data from baseline impact assessment\n",
    "shortest_paths = pickle.load(open(data_path / 'interim' / 'indirect_damages' / 'shortest_paths.pkl', 'rb'))\n",
    "disrupted_edges_by_basin = pickle.load(open(data_path / 'interim' / 'indirect_damages' / 'disrupted_edges_by_basin.pkl', 'rb'))\n",
    "graph_r0 = pickle.load(open(data_path / 'interim' / 'indirect_damages' / 'graph_0.pkl', 'rb'))\n",
    "disrupted_shortest_paths = pickle.load(open(data_path / 'interim' / 'indirect_damages' / 'disrupted_shortest_paths.pkl', 'rb'))\n",
    "event_impacts = pickle.load(open(data_path / 'interim' / 'indirect_damages' / 'event_impacts.pkl', 'rb'))\n",
    "full_flood_event=pickle.load(open(data_path / 'interim' / 'indirect_damages' / 'full_flood_event.pkl', 'rb'))\n",
    "all_disrupted_edges = pickle.load(open(data_path / 'interim' / 'indirect_damages' / 'all_disrupted_edges.pkl', 'rb'))\n",
    "collect_output = pickle.load(open(data_path / 'interim' / 'collected_flood_runs' / 'sample_collected_run.pkl', 'rb'))\n",
    "print('Loaded data from baseline impact assessment')\n",
    "graph_v0=create_virtual_graph(graph_r0)\n",
    "graph_v=graph_v0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define adaptations to apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptations={}\n",
    "adaptations['baseline'] = {'l1_l2_adapt_path': None, 'added_links':[], 'l4_adapt_path': None}\n",
    "adaptations['l1_trib'] = {'l1_l2_adapt_path': data_path/Path('input/adaptations/l1_tributary.geojson'), 'added_links':[], 'l4_adapt_path': None}\n",
    "adaptations['l2_trib'] = {'l1_l2_adapt_path': data_path/Path('input/adaptations/l2_tributary.geojson'), 'added_links':[], 'l4_adapt_path': None}\n",
    "adaptations['l3_trib'] = {'l1_l2_adapt_path': None, 'added_links':[(219651487, 111997047)], 'l4_adapt_path': None}\n",
    "adaptations['l4_trib'] = {'l1_l2_adapt_path': None, 'added_links':[], 'l4_adapt_path': data_path/Path('input/adaptations/l4_tributary.geojson')}\n",
    "\n",
    "# Other adaptations can be added\n",
    "# adaptations['l1_rhine'] = {'l1_l2_adapt_path': data_path/r'input\\adaptations\\l1_rhine.geojson', 'added_links':[], 'l4_adapt_path': None}\n",
    "# adaptations['l2_rhine'] = {'l1_l2_adapt_path': data_path/r'input\\adaptations\\l2_rhine.geojson', 'added_links':[], 'l4_adapt_path': None}\n",
    "# adaptations['l3_rhine'] = {'l1_l2_adapt_path': None, 'added_links':[(112044105, 110947346)], 'l4_adapt_path': None}\n",
    "# adaptations['l4_rhine'] = {'l1_l2_adapt_path': None, 'added_links':[], 'l4_adapt_path': data_path/r'input\\adaptations\\l4_rhine.geojson'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Apply adaptations and recalculate risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5 scenarios:\n",
      "-  baseline\n",
      "-  l1_trib\n",
      "-  l2_trib\n",
      "-  l3_trib\n",
      "-  l4_trib\n",
      "Adaptation baseline already processed. Skipping.\n",
      "Adaptation l1_trib already processed. Skipping.\n",
      "Adaptation l2_trib already processed. Skipping.\n",
      "Adaptation l3_trib already processed. Skipping.\n",
      "Adaptation l4_trib already processed. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# Store results in dictionaries\n",
    "direct_damages_adapted_dict = {}\n",
    "indirect_damages_adapted_dict = {}\n",
    "indirect_damages_adapted_full_dict = {}\n",
    "adaptation_costs={}\n",
    "adapted_assets_dict = {}\n",
    "\n",
    "# Print adaptations that will be run\n",
    "print(f\"Processing {len(adaptations)} scenarios:\")\n",
    "for adapt_id in adaptations.keys():\n",
    "    print('- ',adapt_id)\n",
    "\n",
    "for adapt_id in adaptations.keys():\n",
    "    adaptations_df_path = data_path / 'interim' / 'adaptations' / f'{adapt_id}_adaptations.csv'\n",
    "\n",
    "    if adaptations_df_path.exists():\n",
    "        print(f\"Adaptation {adapt_id} already processed. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Reset graph\n",
    "    graph_v=graph_v0.copy()\n",
    "\n",
    "    # Load adaptations dictionary to the relevant variables\n",
    "    l1_l2_adapt_path = adaptations[adapt_id]['l1_l2_adapt_path']\n",
    "    added_links = adaptations[adapt_id]['added_links']  \n",
    "    l4_adapt_path = adaptations[adapt_id]['l4_adapt_path']\n",
    "\n",
    "    # Load adaptation data\n",
    "    if l1_l2_adapt_path is not None:\n",
    "        adapted_area = gpd.read_file(l1_l2_adapt_path).to_crs(3857)\n",
    "    else:\n",
    "        adapted_area = None\n",
    "    if l4_adapt_path is not None:\n",
    "        adapted_route_area = gpd.read_file(l4_adapt_path).to_crs(3857)\n",
    "    else:\n",
    "        adapted_route_area = None\n",
    "\n",
    "    # Apply adaptations\n",
    "    adapted_assets, adaptations_df, demand_reduction_dict, l3_adaptation_costs = apply_adaptations(adapted_area, assets, collect_output, interim_data_path, rp_spec_priority, adaptation_unit_costs, shortest_paths, graph_v, added_links, adapted_route_area)\n",
    "\n",
    "    # Calculate l1 adaptation costs\n",
    "    local_haz_path=Path(data_path / 'Floods/Germany/basin_intersections')\n",
    "    l1_adaptation_costs=calculate_l1_costs(local_haz_path, interim_data_path, adapted_area, adaptation_unit_costs, adapted_assets) \n",
    "\n",
    "    # Run adapted damages for individual hazard maps\n",
    "    direct_damages_adapted, indirect_damages_adapted, adaptation_run_full, l2_adaptation_costs, overlay_assets_lists = run_adapted_damages(data_path, config_file, collect_output, disrupted_edges_by_basin, interim_data_path, assets, geom_dict, adapted_assets, adaptations_df, rp_spec_priority, adaptation_unit_costs, shortest_paths, graph_v, average_train_load_tons, average_train_cost_per_ton_km, average_road_cost_per_ton_km, demand_reduction_dict)\n",
    "\n",
    "    # Run adapted damages for full flood event\n",
    "    indirect_damages_adapted_full = calculate_indirect_dmgs_fullflood(full_flood_event, overlay_assets_lists, adaptation_run_full, assets, all_disrupted_edges, shortest_paths, graph_v, average_train_load_tons, average_train_cost_per_ton_km, average_road_cost_per_ton_km, demand_reduction_dict)\n",
    "\n",
    "    # Fill in missing values in dictionaries\n",
    "    for hazard_map in collect_output.keys():\n",
    "        if direct_damages_adapted[hazard_map]=={}:\n",
    "            direct_damages_adapted[hazard_map]=collect_output[hazard_map]\n",
    "        if indirect_damages_adapted[hazard_map]=={}:\n",
    "            indirect_damages_adapted[hazard_map]=event_impacts[hazard_map] if hazard_map in event_impacts.keys() else 0.0\n",
    "    \n",
    "    # Store results in dictionaries\n",
    "    direct_damages_adapted_dict[adapt_id] = direct_damages_adapted\n",
    "    indirect_damages_adapted_dict[adapt_id] = indirect_damages_adapted\n",
    "    indirect_damages_adapted_full_dict[adapt_id] = indirect_damages_adapted_full\n",
    "    adapted_assets_dict[adapt_id] = adapted_assets\n",
    "    adaptation_costs[adapt_id] = {'l1': l1_adaptation_costs, \n",
    "                                  'l2': l2_adaptation_costs, \n",
    "                                  'l3': l3_adaptation_costs}\n",
    "    adaptations_df.to_csv(data_path / 'interim' / 'adaptations' / f'{adapt_id}_adaptations.csv')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save raw output to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Report output dataframe\n",
    "output_df = pd.DataFrame.from_dict([direct_damages_adapted_dict, indirect_damages_adapted_dict, indirect_damages_adapted_full_dict, adapted_assets_dict, adaptation_costs])\n",
    "output_df.to_csv(data_path / 'output' / 'adaptations' / 'adaptations_output.csv')\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for adapt_id in adaptations.keys():\n",
    "    if not adapt_id in direct_damages_adapted_dict.keys():\n",
    "        continue\n",
    "    direct_damages_adapted_path = data_path / 'interim' / f'adapted_direct_damages_{adapt_id}.pkl'\n",
    "    indirect_damages_adapted_path = data_path / 'interim' / f'adapted_indirect_damages_{adapt_id}.pkl'\n",
    "    indirect_damages_adapted_full_path = data_path / 'interim' / f'adapted_indirect_damages_full_{adapt_id}.pkl'\n",
    "    # adaptations_df_path = data_path / 'output' / f'adaptations_{adapt_id}.csv'\n",
    "    adapted_assets_path = data_path / 'interim' / f'adapted_assets_{adapt_id}.pkl'\n",
    "    adaptation_costs_path = data_path / 'interim' / f'adaptation_costs_{adapt_id}.pkl'\n",
    "    adaptations_csv_path = data_path / 'interim' / 'adaptations' / f'{adapt_id}_adaptations.csv'\n",
    "\n",
    "\n",
    "    adaptations_df = pd.DataFrame.from_dict(adaptations[adapt_id])\n",
    "    \n",
    "\n",
    "    with open(direct_damages_adapted_path, 'wb') as f:\n",
    "        pickle.dump(direct_damages_adapted_dict[adapt_id], f)\n",
    "    with open(indirect_damages_adapted_path, 'wb') as f:\n",
    "        pickle.dump(indirect_damages_adapted_dict[adapt_id], f)\n",
    "    with open(indirect_damages_adapted_full_path, 'wb') as f:\n",
    "        pickle.dump(indirect_damages_adapted_full_dict[adapt_id], f)    \n",
    "    with open(adapted_assets_path, 'wb') as f:\n",
    "        pickle.dump(adapted_assets_dict[adapt_id], f)\n",
    "    with open(adaptation_costs_path, 'wb') as f:\n",
    "        pickle.dump(adaptation_costs[adapt_id], f)\n",
    "    print(f'Saved results for adaptation: {adapt_id}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
