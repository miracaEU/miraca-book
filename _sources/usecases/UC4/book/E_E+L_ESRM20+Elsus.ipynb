{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5f83ef7-29c2-4050-8eb0-041ede349e99",
   "metadata": {},
   "source": [
    "# UC4\n",
    "\n",
    "UC4 (Adaptation to societal risk of geohazards through schools and hospitals in Greece) will highlight the importance of incorporating geohazards, such as earthquakes and landslides, and hydrological hazards (floods) into climate adaptation strategies. Climate change can intensify extreme weather conditions, which may subsequently increase the frequency and severity of geohazards (e.g., landslides) and hydrological hazards (e.g., floods). UC4 will assess the societal and economic risk of healthcare and educational facilities in Central Macedonia, Greece, under varying climate change scenarios. By evaluating both single-hazard and multi-hazard events, UC4 will contribute to shaping effective adaptation strategies to mitigate their impacts to critical infrastructures, like schools and hospitals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862b6703-b5fd-48f4-8707-01b0595cda7b",
   "metadata": {},
   "source": [
    "## Single hazard - Seismic risk assessment\n",
    "\n",
    "- Use of the newly developed open-access European Seismic Hazard maps (ESHM20, Danciu, et al., 2021) which provide PGA and spectral accelerations at different periods (T=0.3s, 0.6s, 1s, 1.5s) at bedrock for six different return periods, i.e.,  Tm= 73 years, 102 years, 475 years, 1000 years, 2500 years, 5000 years) derived from probabilistic seismic hazard analysis (PSHA)\n",
    "- Estimate median PGA and spectral accelerations at different periods (Sa(T)) at the ground surface for the different return periods considering a detailed site model using Openquake engine \n",
    "- Exposure model: based on GEM building taxonomy \n",
    "- Vulnerability assessment: Use of appropriate seismic vulnerability curves for schools and hospitals (e.g., ESRM20) and appropriate literature curves for concrete precast buildings (Yesilyurt et al. 2021)\n",
    "- Seismic risk assessment : Results will be given in terms of loss maps for different return periods and loss exceedance curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e541f-7e07-4d28-bbbe-01ab48c6ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# HIDE CODE\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from shapely.geometry import Point\n",
    "from scipy.stats import lognorm\n",
    "from rasterstats import point_query\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib import font_manager\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from rasterio.features import shapes\n",
    "from matplotlib.colors import ListedColormap\n",
    "import gc \n",
    "from pathlib import Path\n",
    "from shapely import wkt\n",
    "\n",
    "\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66633b0d-9683-4bbc-9930-14f57d783ac4",
   "metadata": {},
   "source": [
    "\n",
    "Select whether you want to compute the risk assessment for educational buildings (asset = \"Schools\") or healthcare facilities (asset = \"Hospitals\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712243bc-80a9-4d08-ab83-f3f2fbf442d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset = \"Schools\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c6274-58cd-4804-8b1a-c58d5d3b8a48",
   "metadata": {},
   "source": [
    "Define the folder where the data is available and where the results will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563251d-e028-449f-9b04-d2e2b512efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = Path(os.getcwd()) / 'data'\n",
    "\n",
    "savepath = Path.home() / 'UC4' / 'Results' / asset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13806de3-db3e-435b-a934-e9a4e41f3919",
   "metadata": {},
   "source": [
    "The following color variables are defined for the Miraca theme. These colors will be used for visualizations. The font settings for the plots are also adjusted for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a568345-52a1-4f3b-aabe-de47878111a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# miraca colors\n",
    "c1 = '#4069F6' # primary blue 500 \n",
    "c2 = '#171E37' # black\n",
    "c3 = '#64F4C0' # accent green \n",
    "c4 = '#FFFFFF' # white\n",
    "c5 = '#ED5861' # red\n",
    "c6 = '#F8CD48' # yellow\n",
    "c7 = '#72DA95' # green 500\n",
    "c8 = '#373D52' # grey 900\n",
    "c9 = '#8F94A3' # grey 500\n",
    "c10 = '#EBEDF5' # grey 100\n",
    "c11 = '#72DA95' # green\n",
    "c12 = '#373D52' # blue 900\n",
    "c13 = '#6687F8' # blue 400\n",
    "c14 = '#EBEDF5' # blue 100\n",
    "c15 = '#373D52' # 429787 900\n",
    "c16 = '#9CF8D7' # green 400\n",
    "c17 = '#E0FDF2' # green 100\n",
    "\n",
    "# Adjust font settings\n",
    "mpl.rc('font', family='Calibri')\n",
    "font = {'family': 'Calibri', 'weight': 'bold'}\n",
    "rcParams['mathtext.default'] = 'regular'\n",
    "rcParams['mathtext.rm'] = font['family']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e5e15a-bb07-4052-b304-cbe6de7abcba",
   "metadata": {},
   "source": [
    "This dictionary initializes a DataFrame containing seismic return period data. The 'No' column represents a sequential identifier for each seismic scenario. The 'Return_Period' column specifies the return period in years, and the '1/years' column provides the corresponding annual probability of occurrence (i.e., the inverse of the return period). The resulting DataFrame, Scenaria, captures these seismic scenario details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f1505-115c-44fd-920a-accf1c4602d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'No': [0,1,2,3,4,5],\n",
    "        'Return_Period': [73,102,475,1000,2500,5000],\n",
    "        '1/years': [0.0137,0.0098,0.0021,0.001,0.0003,0.0001]\n",
    "}\n",
    "\n",
    "Scenaria = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe7de9-f9bd-48ab-a5b4-8caca11e5949",
   "metadata": {},
   "source": [
    "Load & visualise exposure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e51b11-18e9-4373-84fa-58b2f5c064a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load exposure data\n",
    "if asset == \"Schools\":\n",
    "    csv1_path = data_path / \"Exposure_Riskschools.csv\"\n",
    "else:\n",
    "    csv1_path = data_path / \"Exposure_Hospitals.csv\"\n",
    "\n",
    "\n",
    "# Load the CSV file into pandas DataFrames\n",
    "df1= pd.read_csv(csv1_path)\n",
    "\n",
    "# Column names for longitude and latitude\n",
    "longitude_col1 = 'Longitude'\n",
    "latitude_col1 = 'Latitude'\n",
    "\n",
    "# Convert the DataFrame to GeoDataFrames\n",
    "gdf1 = gpd.GeoDataFrame(df1, geometry=gpd.points_from_xy(df1[longitude_col1], df1[latitude_col1]))\n",
    "\n",
    "# Set the CRS  \n",
    "gdf1.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "gdf1.explore(\n",
    "    marker_kwds={\n",
    "        \"radius\": 4,  # Scale marker size (adjust as needed)\n",
    "        \"fill\": True\n",
    "    },\n",
    "    color=c1,                         # Color map: Orange-Red (you can change it)\n",
    "    legend=True                          # Show legend\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6551b59-788a-4f46-a9cd-dedc9c52d758",
   "metadata": {},
   "source": [
    "Load & visualise hazard data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab4d5d-da18-45cd-b542-f34af95a5b2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load hazard data\n",
    "csv2_path = data_path / \"Hazard_Central_Macedonia.csv\"\n",
    "\n",
    "# Load the CSV file into pandas DataFrames\n",
    "df2 = pd.read_csv(csv2_path)\n",
    "\n",
    "# Column names for longitude and latitude\n",
    "longitude_col2 = 'lon'\n",
    "latitude_col2 = 'lat'\n",
    "\n",
    "# Convert the DataFrame to GeoDataFrames\n",
    "gdf2 = gpd.GeoDataFrame(df2, geometry=gpd.points_from_xy(df2[longitude_col2], df2[latitude_col2]))\n",
    "\n",
    "# Set the CRS \n",
    "gdf2.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "\n",
    "gdf2.explore(\n",
    "    column=\"PGA-0.001\",\n",
    "    cmap=\"OrRd\",\n",
    "    legend=True,\n",
    "    legend_kwds={\n",
    "        'caption': 'PGA (T=1000 years)',\n",
    "        'orientation': 'horizontal'  # optional: 'horizontal' or 'vertical'\n",
    "    },\n",
    "    marker_kwds={\n",
    "        'radius': 4,\n",
    "        'fill': True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242b906-9511-4a98-aec3-8aea1b80c5e0",
   "metadata": {},
   "source": [
    "The follwing csv file contains the spatially joined data of exposure and hazard points, with the nearest hazard point coordinates added for each exposure point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833b01f-6f7d-43c7-aab0-c55cc7688afd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform the spatial join using the nearest neighbor method\n",
    "joined_gdf = gpd.sjoin_nearest(gdf1, gdf2, how=\"left\", distance_col=\"distance\")\n",
    "\n",
    "# Add nearest x and y coordinates from gdf2\n",
    "\n",
    "joined_gdf['nearest_x'] = joined_gdf[longitude_col2]  # Nearest longitude from gdf2\n",
    "joined_gdf['nearest_y'] = joined_gdf[latitude_col2]   # Nearest latitude from gdf2\n",
    "\n",
    "# Ensure the directory exists before saving\n",
    "savepath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the joined GeoDataFrame to CSV\n",
    "joined_gdf.to_csv(savepath / f\"Joined_nearest_hazard_{asset}.csv\", index=False)\n",
    "\n",
    "\n",
    "joined_gdf.explore(\n",
    "    column=\"PGA-0.001\",\n",
    "    cmap=\"OrRd\",\n",
    "    legend=True,\n",
    "    legend_kwds={\n",
    "        'caption': 'PGA (T=1000 years)',\n",
    "        'orientation': 'horizontal'  # optional: 'horizontal' or 'vertical'\n",
    "    },\n",
    "    marker_kwds={\n",
    "        'radius': 4,\n",
    "        'fill': True\n",
    "    },\n",
    "    tooltip=\"PGA-0.001\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ea2f33-e87f-426d-86dd-b189c2769ab0",
   "metadata": {},
   "source": [
    "Load the the European Landslide Susceptibility Map version 2 for the region of central Macedonia in Greece (already converting raster data into polygons - precomputed data). Also, it was performed spatial join to find the nearest raster values by linking each exposure point with the nearest raster values from the landslide susceptibility map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed93fe-2dc1-4700-9207-ec35cbd39820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV\n",
    "csv3_path = data_path / f\"Joined_nearest_susceptibility_{asset}.csv\"\n",
    "\n",
    "susc= pd.read_csv(csv3_path)\n",
    "\n",
    "# Convert WKT string to geometry (assuming the column is named 'geometry')\n",
    "susc['geometry'] = susc['geometry'].apply(wkt.loads)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "joined_gdf = gpd.GeoDataFrame(susc, geometry='geometry')\n",
    "\n",
    "# Set the correct CRS if known (replace EPSG:4326 if different)\n",
    "joined_gdf.set_crs(\"EPSG:4326\", inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11265b2-1341-4a23-b7c5-5d5f168a2024",
   "metadata": {},
   "source": [
    "Visualise susceptibility map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4945c436-ed8d-44cd-898d-b9eafd6a1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure raster_value is treated as categorical with specific order\n",
    "joined_gdf[\"Susceptibility_index\"] = pd.Categorical(\n",
    "    joined_gdf[\"raster_value\"],\n",
    "    categories=[1, 2, 3, 4, 5],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Step 2: Define custom colors for each class (adjust as needed)\n",
    "colors = ['#ffffcc', '#a1dab4', '#41b6c4', '#2c7fb8', 'red']  # Light yellow to dark blue\n",
    "\n",
    "# Step 3: Plot with .explore using the categorical column and custom colors\n",
    "joined_gdf.explore(\n",
    "    column=\"Susceptibility_index\",\n",
    "    #cmap=ListedColormap(colors),\n",
    "    cmap=\"OrRd\",\n",
    "    legend=True,\n",
    "    legend_kwds={\n",
    "        'caption': 'Susceptibility Index',\n",
    "        'orientation': 'horizontal'\n",
    "    },\n",
    "    marker_kwds={\n",
    "        'radius': 4,\n",
    "        'fill': True,\n",
    "    },\n",
    "    text=\"Susceptibility_index\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690da796-fcd4-4186-8a85-096c6145eb08",
   "metadata": {},
   "source": [
    "Final dataframe containing exposure and hazard (earthquake and landslides) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c602e-01de-49bb-bf7a-cebf260bc295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Exposure and Hazard Data\n",
    "df = pd.read_csv(savepath / f\"Joined_nearest_hazard_{asset}.csv\")\n",
    "# Load the CSV file with the new column\n",
    "new_column_df = pd.read_csv(data_path / f\"Joined_nearest_susceptibility_{asset}.csv\")\n",
    "df['SAMPLE_1'] = new_column_df['raster_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df9a68-8162-49f2-8d0c-a6d083bf70bb",
   "metadata": {},
   "source": [
    "Extract values for fragility based on Typology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d9ee8-6443-46ce-8386-595bfd965a81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load ESRM20 data\n",
    "sheet_name = \"Final\"\n",
    "Fragility = pd.read_excel(data_path / \"fragility_various_IM_lognormal.xlsx\", sheet_name=sheet_name)\n",
    "Fragility.columns = ['Typology', 'IMT', 'Median_DS1', 'Median_DS2', 'Median_DS3', 'Median_DS4', 'Beta_DS1', \"Beta_DS2\", \"Beta_DS3\",\"Beta_DS4\"]\n",
    "\n",
    "# Initialize lists to store results\n",
    "IM1 = []\n",
    "IM2 = []\n",
    "IM3 = []\n",
    "IM4 = []\n",
    "Beta_DS1 = []\n",
    "Beta_DS2 = []\n",
    "Beta_DS3 = []\n",
    "Beta_DS4 = []\n",
    "\n",
    "# Iterate over each value in 'Typology' column of data\n",
    "for taxonomy in df['Typology']:\n",
    "    # Check if any row in 'Typology' column matches current taxonomy\n",
    "    if (Fragility['Typology'] == taxonomy).any():\n",
    "        # Find the row where 'Typology' matches and retrieve values\n",
    "        filtered_data = Fragility.loc[Fragility['Typology'] == taxonomy]\n",
    "        if not filtered_data.empty:\n",
    "            IM1.append(filtered_data['Median_DS1'].values[0])\n",
    "            IM2.append(filtered_data['Median_DS2'].values[0])\n",
    "            IM3.append(filtered_data['Median_DS3'].values[0])\n",
    "            IM4.append(filtered_data['Median_DS4'].values[0])\n",
    "            Beta_DS1.append(filtered_data['Beta_DS1'].values[0])\n",
    "            Beta_DS2.append(filtered_data['Beta_DS2'].values[0])\n",
    "            Beta_DS3.append(filtered_data['Beta_DS3'].values[0])\n",
    "            Beta_DS4.append(filtered_data['Beta_DS4'].values[0])\n",
    "        else:\n",
    "            # Handle case where no matching Typology is found in ESRM20\n",
    "            IM1.append(None)\n",
    "            IM2.append(None)\n",
    "            IM3.append(None)\n",
    "            IM4.append(None)\n",
    "            Beta_DS1.append(None)\n",
    "            Beta_DS2.append(None)\n",
    "            Beta_DS3.append(None)            \n",
    "            Beta_DS4.append(None)\n",
    "    else:\n",
    "        # Handle case where no matching Typology is found in ESRM20\n",
    "        IM1.append(None)\n",
    "        IM2.append(None)\n",
    "        IM3.append(None)\n",
    "        IM4.append(None)\n",
    "        Beta_DS1.append(None)\n",
    "        Beta_DS2.append(None)\n",
    "        Beta_DS3.append(None)            \n",
    "        Beta_DS4.append(None)\n",
    "\n",
    "# Create a new DataFrame to store results\n",
    "IM = pd.DataFrame({\n",
    "    'IM1': IM1,\n",
    "    'IM2': IM2,\n",
    "    'IM3': IM3,\n",
    "    'IM4': IM4,\n",
    "    'Beta_DS1': Beta_DS1,\n",
    "    'Beta_DS2': Beta_DS2,\n",
    "    'Beta_DS3': Beta_DS3 ,   \n",
    "    'Beta_DS4': Beta_DS4\n",
    "})\n",
    "\n",
    "IM.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a5419-fde6-4ae6-a917-8b7e375a817a",
   "metadata": {},
   "source": [
    "Calculate damage index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2a3445-baf8-4f17-bcaa-bf93ea217370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_normal_distribution(beta, scale):\n",
    "    \"\"\"\n",
    "    Creates a log-normal distribution using the given beta and scale parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    - beta: Beta parameter (shape) for the log-normal distribution.\n",
    "    - scale: Scale parameter for the log-normal distribution (IM value).\n",
    "    \n",
    "    Returns:\n",
    "    - dist: A log-normal distribution object.\n",
    "    \"\"\"\n",
    "    return lognorm(s=beta, scale=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25276fd7-f613-4497-ab9c-4a4941f2c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beta_values(typology, im_row):\n",
    "    \"\"\"\n",
    "    Determines beta values based on typology. Adjusts beta values if the typology contains 'Precast'.\n",
    "    \n",
    "    Parameters:\n",
    "    - typology: The building typology.\n",
    "    - im_row: The current row from the IM DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of beta values for DS1, DS2, DS3, and DS4.\n",
    "    \"\"\"\n",
    "    if 'Precast' in typology:\n",
    "        beta_dist1 = im_row['Beta_DS1'] if pd.notna(im_row['Beta_DS1']) else im_row['Beta_DS1']\n",
    "        beta_dist2 = im_row['Beta_DS2'] if pd.notna(im_row['Beta_DS2']) else im_row['Beta_DS1']\n",
    "        beta_dist3 = im_row['Beta_DS3'] if pd.notna(im_row['Beta_DS3']) else im_row['Beta_DS1']\n",
    "        beta_dist4 = im_row['Beta_DS4'] if pd.notna(im_row['Beta_DS4']) else im_row['Beta_DS1']\n",
    "    else:\n",
    "        beta_dist1 = im_row['Beta_DS1']\n",
    "        beta_dist2 = im_row['Beta_DS1']\n",
    "        beta_dist3 = im_row['Beta_DS1']\n",
    "        beta_dist4 = im_row['Beta_DS1']\n",
    "    \n",
    "    return beta_dist1, beta_dist2, beta_dist3, beta_dist4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141c611-43fd-4728-ade4-204ec3761e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cdf_values(row, intensity_column_map, im_row):\n",
    "    \"\"\"\n",
    "    Computes CDF values based on the intensity and IM row data.\n",
    "    \n",
    "    Parameters:\n",
    "    - row: A row from the input DataFrame.\n",
    "    - intensity_column_map: A map that links intensity types to column names.\n",
    "    - im_row: A row from the IM DataFrame containing beta and IM values.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple of computed CDF values (cdf1, cdf2, cdf3, cdf4).\n",
    "    \"\"\"\n",
    "    # Get the intensity type and column name for current row\n",
    "    intensity_type = row[\"Intensity\"]\n",
    "    column_name = intensity_column_map.get(intensity_type)\n",
    "    x_value = row[column_name] if column_name else np.nan\n",
    "\n",
    "    # Calculate log-normal distributions\n",
    "    beta_dist1, beta_dist2, beta_dist3, beta_dist4 = get_beta_values(row['Typology'], im_row)\n",
    "    \n",
    "    dist1 = calculate_log_normal_distribution(beta_dist1, im_row[\"IM1\"])\n",
    "    dist2 = calculate_log_normal_distribution(beta_dist2, im_row[\"IM2\"])\n",
    "    dist3 = calculate_log_normal_distribution(beta_dist3, im_row[\"IM3\"])\n",
    "    dist4 = calculate_log_normal_distribution(beta_dist4, im_row[\"IM4\"])\n",
    "    \n",
    "    # Compute CDF values\n",
    "    return dist1.cdf(x_value), dist2.cdf(x_value), dist3.cdf(x_value), dist4.cdf(x_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4623bd5-108d-45ab-953d-faadc972a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store DataFrames from each scenario by their years_value\n",
    "cdf_dict = {}\n",
    "\n",
    "# Iterate over each seismic scenario\n",
    "for index_scen, scen_row in Scenaria.iterrows():\n",
    "    # Variables to store CDF results and intensity values\n",
    "    cdf1, cdf2, cdf3, cdf4 = [], [], [], []\n",
    "    \n",
    "    # Scenario parameters\n",
    "    years_value = scen_row['1/years']\n",
    "    scenario_no = int(scen_row['No'])\n",
    "    return_period = int(scen_row['Return_Period'])\n",
    "\n",
    "    # Generate intensity column map for the current scenario\n",
    "    intensity_column_map = {\n",
    "        \"PGA\": f\"PGA-{years_value}\",\n",
    "        \"SA(0.3)\": f\"SA(0.3)-{years_value}\",\n",
    "        \"SA(0.6)\": f\"SA(0.6)-{years_value}\",\n",
    "        \"SA(1.0)\": f\"SA(1.0)-{years_value}\",\n",
    "        \"SA(1.5)\": f\"SA(1.5)-{years_value}\",\n",
    "    }\n",
    "\n",
    "    # Iterate over each row in the DataFrame df\n",
    "    for index, row in df.iterrows():\n",
    "        im_row = IM.iloc[index]  # Corresponding IM DataFrame row\n",
    "    \n",
    "        # Compute CDF values\n",
    "        cdf1_val, cdf2_val, cdf3_val, cdf4_val = compute_cdf_values(row, intensity_column_map, im_row)\n",
    "        cdf1.append(cdf1_val)\n",
    "        cdf2.append(cdf2_val)\n",
    "        cdf3.append(cdf3_val)\n",
    "        cdf4.append(cdf4_val)\n",
    "    \n",
    "    # Create DataFrame to store CDF values for the current scenario\n",
    "    CDF = pd.DataFrame({\n",
    "        f\"CDF1-{years_value}\": cdf1,\n",
    "        f\"CDF2-{years_value}\": cdf2,\n",
    "        f\"CDF3-{years_value}\": cdf3,\n",
    "        f\"CDF4-{years_value}\": cdf4\n",
    "    })\n",
    "    \n",
    "    # Calculate probabilities of occurrence for each damage state\n",
    "    CDF[f\"P_DS1-{years_value}\"] = CDF[f\"CDF1-{years_value}\"] - CDF[f\"CDF2-{years_value}\"]\n",
    "    CDF[f\"P_DS2-{years_value}\"] = CDF[f\"CDF2-{years_value}\"] - CDF[f\"CDF3-{years_value}\"]\n",
    "    CDF[f\"P_DS3-{years_value}\"] = CDF[f\"CDF3-{years_value}\"] - CDF[f\"CDF4-{years_value}\"]\n",
    "    CDF[f\"P_DS4-{years_value}\"] = CDF[f\"CDF4-{years_value}\"]\n",
    "\n",
    "    # Initialize the list for Damage Index (DI) values\n",
    "    DI_values = []\n",
    "\n",
    "    # Loop over the rows again to calculate the DI for each row\n",
    "    for index, row in df.iterrows():\n",
    "        # Check Material type and calculate DI accordingly\n",
    "        if row['Material'] == 'Precast':\n",
    "            DI_value = (\n",
    "                CDF.loc[index, f\"P_DS1-{years_value}\"] * 0.05 +\n",
    "                CDF.loc[index, f\"P_DS2-{years_value}\"] * 0.30 +\n",
    "                CDF.loc[index, f\"P_DS3-{years_value}\"] * 0.70 +\n",
    "                CDF.loc[index, f\"P_DS4-{years_value}\"] * 1\n",
    "            )\n",
    "        else:\n",
    "            DI_value = (\n",
    "                CDF.loc[index, f\"P_DS1-{years_value}\"] * 0.05 +\n",
    "                CDF.loc[index, f\"P_DS2-{years_value}\"] * 0.15 +\n",
    "                CDF.loc[index, f\"P_DS3-{years_value}\"] * 0.60 +\n",
    "                CDF.loc[index, f\"P_DS4-{years_value}\"] * 1\n",
    "            )\n",
    "        \n",
    "        # Append the calculated DI to the list\n",
    "        DI_values.append(DI_value)\n",
    "\n",
    "    # Add the calculated DI values to the DataFrame\n",
    "    CDF[f\"DI-{years_value}\"] = DI_values\n",
    "    \n",
    "    # Reset the index to align all data\n",
    "    CDF.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Store the DataFrame for the current scenario in a dictionary\n",
    "    cdf_dict[years_value] = CDF\n",
    "\n",
    "# Concatenate all the DataFrames horizontally (axis=1)\n",
    "DI_seismic = pd.concat(cdf_dict.values(), axis=1)\n",
    "\n",
    "# Drop duplicate 'Longitude' and 'Latitude' columns, keeping only the first occurrence\n",
    "DI_seismic = DI_seismic.loc[:, ~DI_seismic.columns.duplicated()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6597bb33-328e-4a69-9fce-b8664367b933",
   "metadata": {},
   "source": [
    "Calculate annual collapse probability based on strest methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad07e3d-ef25-47d0-a17e-9742419c06aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define return periods\n",
    "Periods = [5,73,102,475,1000,2500,5000,10000]\n",
    "\n",
    "lamda1=[0.9999,0.5,0.39,0.1,0.05,0.02,0.01,0.005]\n",
    "\n",
    "# Compute λ values\n",
    "lambda_values = [1 / (-50 / np.log(1 - l)) for l in lamda1]\n",
    "\n",
    "\n",
    "# Compute P[IM_i] using the given formula\n",
    "P_IM = []\n",
    "for t in range(1, len(lambda_values) - 1):  # Avoid first and last index\n",
    "    P_IM_i = (lambda_values[t - 1] - lambda_values[t + 1]) / 2\n",
    "    P_IM.append(P_IM_i)\n",
    "\n",
    "# Create a dictionary mapping return periods to P[IM_i]\n",
    "P_IM_dict = {data['1/years'][i]: P_IM[i] for i in range(len(P_IM))}\n",
    "\n",
    "# Multiply each \"CDF4-{r}\" column by the corresponding P[IM_i]\n",
    "for r in data['1/years']:  # Exclude first and last periods as they lack P[IM_i]\n",
    "    column_name = f\"CDF4-{r}\"\n",
    "    new_column_name = f\"{column_name}*P[IM_i]\"\n",
    "\n",
    "    if column_name in DI_seismic.columns:  \n",
    "        DI_seismic[new_column_name] = DI_seismic[column_name] * P_IM_dict[r]\n",
    "\n",
    "# Add a new column that sums all the new columns\n",
    "new_columns = [f\"CDF4-{r}*P[IM_i]\" for r in data['1/years'] if f\"CDF4-{r}\" in DI_seismic.columns]\n",
    "\n",
    "# Create a new column \"Sum_of_new_columns\" by summing across the new columns\n",
    "DI_seismic[\"Annual Collapse Probability\"] = DI_seismic[new_columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee50c2e-62a2-41ae-a1f9-88e2ba8e92b6",
   "metadata": {},
   "source": [
    "Create and save the final DataFrame containing the damage index (loss ratio) for different return periods along with the annual probability of collapse for each exposure point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e606754-27a0-4490-9505-6625b656d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate df and IM horizontally, aligning by their index\n",
    "common_df = pd.concat([df, IM, DI_seismic], axis=1)\n",
    "\n",
    "# Define the path first\n",
    "seismic_file_path = savepath / f\"Single-Hazard/Earthquakes/DI_Earthquakes_ESRM20_{asset}.csv\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "seismic_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "common_df.to_csv(seismic_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e1e48c-6abf-4750-b602-9bfa264af8e9",
   "metadata": {},
   "source": [
    "Visualise loss ratio map for a return period of 1000 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc562f-e698-42b4-899c-03ba4b1efff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_df['geometry'] = common_df.apply(lambda row: Point(row['Longitude'], row['Latitude']), axis=1)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "seismic_gdf = gpd.GeoDataFrame(common_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "# Create bins for the DI-0.001 column\n",
    "bins = [0, 0.1, 0.25, 0.4, 1]  # Define the bin ranges\n",
    "labels = ['0-0.1', '0.1-0.25', '0.25-0.4', '0.4-1']  # Labels for the bins\n",
    "\n",
    "# Assign bin values based on 'DI-0.001'\n",
    "seismic_gdf['DI_binned'] = pd.cut(seismic_gdf['DI-0.001'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "\n",
    "# Interactive map with 4 bins and custom colors\n",
    "seismic_gdf.explore(\n",
    "    column=\"DI_binned\",  # Use the binned DI-0.001 column for coloring\n",
    "    cmap='OrRd',         # Optional colormap, we are using custom colors\n",
    "    stroke=True,         # Add stroke (outline) around markers\n",
    "    line_color=\"black\",  # Set the stroke color to black\n",
    "    legend=True,         # Show the legend\n",
    "    legend_kwds={\n",
    "        'caption': 'Loss ratio (T=1000 years)',  # Legend caption\n",
    "        'orientation': 'horizontal'             # Horizontal legend\n",
    "    },\n",
    "    marker_kwds={\n",
    "        'radius': 5,   # Set marker size (adjust as needed)\n",
    "        'fill': True,   # Fill the markers with color\n",
    "    },\n",
    "    tooltip=\"DI-0.001\",  # Show the DI-0.001 value in the tooltip\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc394bf-e168-4ce1-81a3-4fc85ea56575",
   "metadata": {},
   "source": [
    "Compute human and economic losses based on ESRM20 methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a009c-efe7-4f1c-b9f9-4759c8a499c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your reference data (Excel with probabilities)\n",
    "reference = pd.read_excel(data_path / \"fatality_damage_model_ESRM20.xlsx\")\n",
    "\n",
    "# Create empty lists to store the values\n",
    "P_lethal_DS4 = []\n",
    "Collapse_factor = []\n",
    "P_entrap_day = []\n",
    "P_entrap_night = []\n",
    "P_loss_life = []\n",
    "\n",
    "# For each row in common_df, look up values based on Typology\n",
    "for taxonomy in common_df['Typology']:\n",
    "    match = reference[reference['Typology'] == taxonomy]\n",
    "    if not match.empty:\n",
    "        P_lethal_DS4.append(match['P_lethal-building | DS4'].values[0])\n",
    "        Collapse_factor.append(match['Collapse_factor'].values[0])\n",
    "        P_entrap_day.append(match['P_entrapment_day'].values[0])\n",
    "        P_entrap_night.append(match['P_entrapment_night'].values[0])\n",
    "        P_loss_life.append(match['P_loss-life | entrapment'].values[0])\n",
    "    else:\n",
    "        # If not found, append None or a default\n",
    "        P_lethal_DS4.append(None)\n",
    "        Collapse_factor.append(None)\n",
    "        P_entrap_day.append(None)\n",
    "        P_entrap_night.append(None)\n",
    "        P_loss_life.append(None)\n",
    "\n",
    "# Add the new columns to your DataFrame\n",
    "common_df['P_lethal-building | DS4'] = P_lethal_DS4\n",
    "common_df['Collapse_factor'] = Collapse_factor\n",
    "common_df['P_entrapment_day'] = P_entrap_day\n",
    "common_df['P_entrapment_night'] = P_entrap_night\n",
    "common_df['P_loss-life | entrapment'] = P_loss_life\n",
    "\n",
    "# Iterate through each return period in the Scenaria DataFrame\n",
    "for index, row in Scenaria.iterrows():\n",
    "    return_period = row['1/years']  # Get the '1/years' value\n",
    "\n",
    "    # Create the new column name dynamically, e.g., \"new_column_0.0137\" for the first return period\n",
    "    new_column_name = f\"Fatality_damage_{return_period}\"\n",
    "\n",
    "    # Calculate the new column for each row in common_df\n",
    "    common_df[new_column_name] = (\n",
    "        common_df[f\"P_DS4-{return_period}\"] * common_df['P_lethal-building | DS4'] * \n",
    "        common_df['Collapse_factor'] * common_df['P_loss-life | entrapment'] * common_df['Area'] / 3\n",
    "    )\n",
    "\n",
    "    common_df[new_column_name] = common_df[new_column_name].round(0)\n",
    "\n",
    "# Iterate through each return period in the Scenaria DataFrame\n",
    "for index, row in Scenaria.iterrows():\n",
    "    return_period = row['1/years']  # Get the '1/years' value\n",
    "\n",
    "    # Create the new column name dynamically, e.g., \"new_column_0.0137\" for the first return period\n",
    "    new_column_name = f\"Economic_Losses_{return_period}\"\n",
    "\n",
    "    # Calculate the new column for each row in common_df\n",
    "    common_df[new_column_name] = (common_df['Area'] * 2000 * common_df[f\"DI-{return_period}\"])\n",
    "\n",
    "# Save the updated common_df with the new columns\n",
    "common_df.to_csv(seismic_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be30d3e-3309-43cf-9874-74f419f51526",
   "metadata": {},
   "source": [
    "Visualise economic and human losses for a return period of 1000 years aggragated per municipality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd442f6-730f-4cbc-a002-8a7245d5b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_df['geometry'] = common_df.apply(lambda row: Point(row['Longitude'], row['Latitude']), axis=1)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "seismic_gdf = gpd.GeoDataFrame(common_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "municipalities_gdf = gpd.read_file(data_path / \"oria_dimon.shp\")\n",
    "\n",
    "\n",
    "# Ensure both datasets are in the same CRS\n",
    "if seismic_gdf.crs != municipalities_gdf.crs:\n",
    "    seismic_gdf = seismic_gdf.to_crs(municipalities_gdf.crs)\n",
    "\n",
    "# Remove conflicting columns if they exist\n",
    "for col in ['index_left', 'index_right']:\n",
    "    if col in seismic_gdf.columns:\n",
    "        seismic_gdf = seismic_gdf.drop(columns=col)\n",
    "    if col in municipalities_gdf.columns:\n",
    "        municipalities_gdf = municipalities_gdf.drop(columns=col)\n",
    "\n",
    "# Step 2: Perform spatial join — assign each point in seismic_gdf to a municipality\n",
    "joined = gpd.sjoin(seismic_gdf, municipalities_gdf, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Step 3: Aggregate — Sum Economic Losses and Fatality (Human) Losses per municipality\n",
    "agg = joined.groupby('index_right').agg({\n",
    "    'Economic_Losses_0.001': 'sum',  # Sum the Economic Losses for each municipality\n",
    "    'Fatality_damage_0.001': 'sum'   # Sum Human Losses for each municipality\n",
    "}).rename(columns={\n",
    "    'Economic_Losses_0.001': 'Economic_Losses',\n",
    "    'Fatality_damage_0.001': 'Human Losses'\n",
    "})\n",
    "\n",
    "# Step 4: Join the aggregated data back to the municipalities\n",
    "municipalities_gdf = municipalities_gdf.join(agg)\n",
    "\n",
    "# Step 5: Apply binning for **Economic Losses**\n",
    "economic_bins = [0, 400000, 3430000, 6000000, 8098000, 124540000]  # Define custom bins\n",
    "economic_labels = ['0-400K', '400K-3.4M', '3.4M-6M', '6M-8M', '8M+']  # Labels for the bins\n",
    "\n",
    "# Create a new column with binned values for Economic Losses\n",
    "municipalities_gdf['Economic_Losses_binned'] = pd.cut(municipalities_gdf['Economic_Losses'], bins=economic_bins, labels=economic_labels, right=False)\n",
    "\n",
    "# Step 6: Apply binning for **Human Losses** with explicit inclusion of zero in the bins\n",
    "human_bins = [0, 1, 2, 4, 7, np.inf]  # Bins for Human Losses (0-1, 1-2, etc.)\n",
    "human_labels = ['0-1', '1-2', '2-4', '4-7', '7+']  # Labels for the bins\n",
    "\n",
    "# Create a new column with binned values for Human Losses (ensure '0' is included in the first bin)\n",
    "municipalities_gdf['Fatality_binned'] = pd.cut(municipalities_gdf['Human Losses'], bins=human_bins, labels=human_labels, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f703a46-1703-4e46-8211-5d78a0f3edf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot Economic Losses\n",
    "municipalities_gdf.explore(\n",
    "    column='Economic_Losses_binned',\n",
    "    cmap='Spectral_r',\n",
    "    legend=True,\n",
    "    legend_kwds={\n",
    "        'caption': 'Economic Losses € (T=1000 years)',\n",
    "        'orientation': 'horizontal'\n",
    "    },\n",
    "    tooltip='Economic_Losses'  # Display the binned categories in the polygons\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06acbb0-8997-4edf-9438-5eb2f1e3e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Human Losses\n",
    "municipalities_gdf.explore(\n",
    "    column='Fatality_binned',\n",
    "    cmap='Spectral_r',\n",
    "    legend=True,\n",
    "    legend_kwds={\n",
    "        'caption': 'Human Losses (T=1000 years)',\n",
    "        'orientation': 'horizontal'\n",
    "    },\n",
    "    tooltip='Human Losses'  # Display the binned categories in the polygons\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9fa51b4-8985-4a94-bd35-aaaddac0cf2c",
   "metadata": {},
   "source": [
    "## Multi hazard - Earthquake triggered landslide\n",
    "- Seismic hazard assessment: use of the newly developed open-access European Seismic Hazard maps (ESHM20, Danciu, et al., 2021) which provide PGA and spectral accelerations at different periods at bedrock for six different return periods derived from probabilistic seismic hazard analysis\n",
    "- Estimate PGA at the ground surface for different return periods considering a detailed site model using Openquake engine \n",
    "- Relate ELSUSv2 susceptibility classes (Wilde et al. 2018) with yield acceleration ratio (ky) values based on FEMA (2022) and expert judgement\n",
    "- Assume that the frequency of exceedance of the earthquake induced landslide will be the same as the one of the earthquake itself (a rather conservative assumption) or assess the probability that the landslide will occur given the occurrence of the earthquake.\n",
    "- Estimate Permanent ground displacement (PGD) of (earthquake induced) landsliding using different analytical expressions (e.g., Fotopoulou and Pitilakis 2015; Rathje and Antonakos 2011) that relate the PGD with PGA and yield coefficient (ky). \n",
    "- Use of appropriate fragility curves for schools and hospitals due to ground shaking (e.g., ESRM20) and landslide (FEMA, 2022)\n",
    "- Estimate the compound damages due to ground shaking and ground failure due to landsliding by combining the damage state probabilities for the different hazards (Fotopoulou and Pitilakis 2017, FEMA 2022). The combined probability (PCOMB) of exceeding a given limit state i is given as: PCOMB[LS ≥ LSi] =  PGS[ LS ≥ LSi] +Plandslide· PL[ LS ≥ LSi] - PGS[ LS ≥ LSi] · Plandslide ·PL[ LS ≥ LSi] where Plandslide is the probability of landslide occurrence given the earthquake. \n",
    "- A loss ratio will be finally computed that will be based on the two intensity measures (PGA and PGD).\n",
    "- The results will be given in terms of (economic and human) loss maps for the different return periods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21572b01-450d-4890-8a26-7029edc8b9a1",
   "metadata": {},
   "source": [
    "Calculate yield acceleration ratio (ky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c093c3c-c3a6-442b-b813-93a3ee44d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the logic\n",
    "def calculate_value(sample):\n",
    "    if sample == 1:\n",
    "        return 0.4\n",
    "    elif sample == 2:\n",
    "        return 0.3\n",
    "    elif sample == 3:\n",
    "        return 0.2\n",
    "    elif sample == 4:\n",
    "        return 0.15\n",
    "    elif sample == 5:\n",
    "        return 0.1\n",
    "    else:\n",
    "        return 0  # default value when CW is not 1, 2, 3, 4, or 5\n",
    "\n",
    "# Apply the calculation and create a new dataframe\n",
    "ky_new = {\n",
    "    'ky': df['SAMPLE_1'].apply(lambda x: calculate_value(x)),\n",
    "}\n",
    "\n",
    "ky = pd.DataFrame(ky_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeba31fa-96d3-4107-abcd-4d90a0d64fad",
   "metadata": {},
   "source": [
    "Calculate Permanent ground displacement (PGD) of (earthquake induced) landsliding by using an analytical expression that relates the PGD with PGA: $$PGD = \\exp(a + b \\cdot \\ln(\\text{PGA}) - c \\cdot k_y + 0.535 \\cdot d)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033fc049-4ce9-492d-bb02-823d8aff1769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_PGD(df, PGA_column, return_period):\n",
    "    \"\"\"\n",
    "    This function calculates Permanent Ground Displacement (PGD) for a given return period using\n",
    "    the specified Peak Ground Acceleration (PGA) data.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing PGA values and other relevant data.\n",
    "    PGA_column (str): The name of the column in the DataFrame that contains the PGA values.\n",
    "    return_period (int): The return period in years, which determines the value of constant 'd' used in the calculation.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: A Series containing the calculated PGD values for the specified return period.\n",
    "    \"\"\"\n",
    "    # Constants for the calculation\n",
    "    a = -2.965\n",
    "    b = 2.127\n",
    "    c = 6.583\n",
    "    if return_period == 73 or return_period == 102:\n",
    "        d = 5\n",
    "    elif return_period == 475:\n",
    "        d = 6\n",
    "    else:\n",
    "        d = 7\n",
    "\n",
    "    # Calculate PGD\n",
    "    PGD = np.exp(a + b * np.log(df[PGA_column]) - c * ky[\"ky\"] + 0.535 * d)\n",
    "\n",
    "    return PGD  # Return as a Series instead of a DataFrame\n",
    "\n",
    "# Define return periods and corresponding PGA columns\n",
    "return_periods = [73, 102, 475, 1000, 2500, 5000]\n",
    "PGA_columns = ['PGA-0.0137', 'PGA-0.0098', 'PGA-0.0021', 'PGA-0.001', 'PGA-0.0003', 'PGA-0.0001']\n",
    "\n",
    "# Create a DataFrame to store all PGD results\n",
    "PGD = pd.DataFrame()\n",
    "\n",
    "# Loop through each return period and PGA column\n",
    "for index, row in Scenaria.iterrows():\n",
    "    return_period = row['Return_Period']\n",
    "    PGA_column = f'PGA-{row[\"1/years\"]}'  # Construct the corresponding PGA column name\n",
    "    PGD_value = calculate_PGD(df, PGA_column, return_period)\n",
    "    PGD[f'PGD-{row[\"1/years\"]}'] = PGD_value  # Add the PGD values to the DataFrame\n",
    "\n",
    "PGD.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80aed6a-675f-4c2f-97c8-95011ab51a83",
   "metadata": {},
   "source": [
    "Calculate the probability of the occurrence of the earthquake induced landslide "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dcd3f4-1640-422b-b259-a9900cd7abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract years_values from the Scenaria DataFrame\n",
    "years_values = Scenaria['1/years'].tolist()  # Use the annual probabilities as years values\n",
    "\n",
    "# Create a list to store Planaslide DataFrames for each years_value\n",
    "plandslide_df_list = []\n",
    "\n",
    "# Function to assign Planaslide values based on PGA and Sample\n",
    "def assign_plandslide_values(sample):\n",
    "    if sample == 1:\n",
    "        return 0.000\n",
    "    elif sample == 2:\n",
    "        return 0.010\n",
    "    elif sample == 3:\n",
    "        return 0.050\n",
    "    elif sample == 4:\n",
    "        return 0.100\n",
    "    elif sample == 5:\n",
    "        return 0.400\n",
    "\n",
    "# Initialize a list to store the DataFrames for each year\n",
    "plandslide_df_list = []\n",
    "\n",
    "# Iterate over each years_value\n",
    "for years_value in years_values:\n",
    "    # List to store Planaslide data for this specific years_value\n",
    "    plandslide_data = []\n",
    "\n",
    "    # Iterate over rows in the existing df DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        pga_value = row[f\"PGA-{years_value}\"]  # Access the PGA value using dynamic column name\n",
    "        sample_value = row['SAMPLE_1']  # Get the Sample value\n",
    "        plandslide1 = assign_plandslide_values(sample_value)  # Get Planaslide value\n",
    "\n",
    "        # Store results in a dictionary\n",
    "        plandslide_data.append({\n",
    "            f'Plandslide-{years_value}': f\"{plandslide1:.3f}\",  # Unique column for each year\n",
    "        })\n",
    "\n",
    "    # Create a new DataFrame for this specific years_value\n",
    "    plandslide_df = pd.DataFrame(plandslide_data)\n",
    "\n",
    "\n",
    "    plandslide_df_list.append(plandslide_df)\n",
    "\n",
    "# Concatenate all DataFrames into one common DataFrame along columns (axis=1)\n",
    "Plandslide_df = pd.concat(plandslide_df_list, axis=1)\n",
    "\n",
    "Plandslide_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1572e5-943d-42cd-a59b-c34abb691264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_landcdf_values(row, years_value):\n",
    "    \"\"\"\n",
    "    Computes CDF values based on the PGD values.\n",
    "    \n",
    "    Parameters:\n",
    "    - row: A row from the input DataFrame containing PGD values.\n",
    "    - years_value: The years value to access the correct PGD column.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple of computed CDF values (cdf1, cdf2, cdf3, cdf4).\n",
    "    \"\"\"\n",
    "    # Access the PGD value from the dynamic column\n",
    "    pga_column_name = f\"PGD-{years_value}\"  # Create the column name based on years_value\n",
    "    pgd_value = row[pga_column_name]  # Access the PGD value\n",
    "\n",
    "    # Fixed beta and scale values\n",
    "    beta = 1.2\n",
    "    scale_values = [0.254] * 5  # Four IM values, all set to 0.254\n",
    "\n",
    "    # Calculate log-normal distributions with fixed values\n",
    "    dist1 = calculate_log_normal_distribution(beta, scale_values[0])\n",
    "    dist2 = calculate_log_normal_distribution(beta, scale_values[1])\n",
    "    dist3 = calculate_log_normal_distribution(beta, scale_values[2])\n",
    "    dist4 = calculate_log_normal_distribution(beta, scale_values[3])\n",
    "\n",
    "    # Compute CDF values using PGD as the x_value\n",
    "    landcdf1 = dist1.cdf(pgd_value) if pd.notnull(pgd_value) else np.nan\n",
    "    landcdf2 = dist2.cdf(pgd_value) if pd.notnull(pgd_value) else np.nan\n",
    "    landcdf3 = dist3.cdf(pgd_value) if pd.notnull(pgd_value) else np.nan\n",
    "    landcdf4 = dist4.cdf(pgd_value) if pd.notnull(pgd_value) else np.nan\n",
    "\n",
    "    return landcdf1, landcdf2, landcdf3, landcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af96a49-521c-4ca5-a6f4-c3250e81e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the final results\n",
    "Landcdf = pd.DataFrame()\n",
    "\n",
    "# Iterate over each return period in the Scenaria DataFrame\n",
    "for index, row in Scenaria.iterrows():\n",
    "    years_value = row['1/years']  # Get the years value from the current row\n",
    "    cdf_results = []\n",
    "\n",
    "    # Compute CDF values for each row in the PGD DataFrame\n",
    "    for _, pgd_row in PGD.iterrows():\n",
    "        cdf_values = compute_landcdf_values(pgd_row, years_value)\n",
    "        cdf_results.append(cdf_values)\n",
    "\n",
    "    # Convert the results to a DataFrame\n",
    "    cdf_df = pd.DataFrame(cdf_results, columns=[f'LandDS1-{years_value}', f'LandDS2-{years_value}', f'LandDS3-{years_value}', f'LandDS4-{years_value}'])\n",
    "    \n",
    "    # Concatenate the results to the main landcdf DataFrame\n",
    "    Landcdf = pd.concat([Landcdf, cdf_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe86a1d-b548-486c-bb5c-73e1882efd5e",
   "metadata": {},
   "source": [
    "### PL = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c62ce0-6c77-4394-b5af-ef8a3bc7df27",
   "metadata": {},
   "source": [
    "Calculate the damage index (loss ratio) assuming that the probability of an earthquake-induced landslide is equal to the probability of the earthquake itself (i.e., $P_L = 1.0$), meaning the landslide will certainly occur given that the earthquake occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5723f486-abf5-48cd-8bf5-3f14e0664ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relevant columns to numeric if they are not already\n",
    "DI_seismic = DI_seismic.apply(pd.to_numeric, errors='coerce')\n",
    "Landcdf = Landcdf.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Extract years_values from the Scenaria DataFrame\n",
    "years_values = Scenaria['1/years'].tolist()  # Use the annual probabilities as years values\n",
    "\n",
    "# Initialize a new DataFrame for combined results\n",
    "DI_combined_P1 = pd.DataFrame({\n",
    "})\n",
    "\n",
    "# List of dataset names (DS1, DS2, DS3, DS4.ds5)\n",
    "datasets = [1,2,3,4]\n",
    "\n",
    "# Loop through each years_value and perform the calculations for each dataset\n",
    "for years_value in years_values:\n",
    "    for dataset in datasets:\n",
    "        combined_value = (\n",
    "            DI_seismic[f\"CDF{dataset}-{years_value}\"] + \n",
    "            1 * Landcdf[f\"LandDS{dataset}-{years_value}\"] -\n",
    "            DI_seismic[f\"CDF{dataset}-{years_value}\"] * 1 * Landcdf[f\"LandDS{dataset}-{years_value}\"]\n",
    "        )\n",
    "        \n",
    "        # Store the results in the new DataFrame\n",
    "        DI_combined_P1[f\"CombDS{dataset}-{years_value}\"] = combined_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1ef46-e962-4723-9e0b-90fda3a7dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each years_value and perform the calculations for each dataset\n",
    "for years_value in years_values:\n",
    "    # Calculate probabilities of occurrence for each damage state\n",
    "    DI_combined_P1[f\"P_DS1-{years_value}\"] = DI_combined_P1[f\"CombDS1-{years_value}\"] - DI_combined_P1[f\"CombDS2-{years_value}\"]\n",
    "    DI_combined_P1[f\"P_DS2-{years_value}\"] = DI_combined_P1[f\"CombDS2-{years_value}\"] - DI_combined_P1[f\"CombDS3-{years_value}\"]\n",
    "    DI_combined_P1[f\"P_DS3-{years_value}\"] = DI_combined_P1[f\"CombDS3-{years_value}\"] - DI_combined_P1[f\"CombDS4-{years_value}\"]\n",
    "    DI_combined_P1[f\"P_DS4-{years_value}\"] = DI_combined_P1[f\"CombDS4-{years_value}\"]\n",
    "\n",
    "    # Calculate damage index for the scenario based on Typology\n",
    "    for index in range(len(DI_combined_P1)):\n",
    "        if df['Material'].iloc[index] == 'Precast':\n",
    "            DI_combined_P1.at[index, f\"DI-{years_value}\"] = (\n",
    "                DI_combined_P1.at[index, f\"P_DS1-{years_value}\"] * 0.05 +\n",
    "                DI_combined_P1.at[index, f\"P_DS2-{years_value}\"] * 0.30 +\n",
    "                DI_combined_P1.at[index, f\"P_DS3-{years_value}\"] * 0.70 +\n",
    "                DI_combined_P1.at[index, f\"P_DS4-{years_value}\"] * 1\n",
    "            )\n",
    "        else:\n",
    "            DI_combined_P1.at[index, f\"DI-{years_value}\"] = (\n",
    "                DI_combined_P1.at[index, f\"P_DS1-{years_value}\"] * 0.05 +\n",
    "                DI_combined_P1.at[index, f\"P_DS2-{years_value}\"] * 0.15 +\n",
    "                DI_combined_P1.at[index, f\"P_DS3-{years_value}\"] * 0.60 +\n",
    "                DI_combined_P1.at[index, f\"P_DS4-{years_value}\"] * 1\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5ed16-8324-4c37-9fe8-79705d5d58cd",
   "metadata": {},
   "source": [
    "Calculate annual collapse probability based on strest methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded10f99-19a9-4f88-9020-f19f079c7ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define return periods\n",
    "Periods = [5,73,102,475,1000,2500,5000,10000]\n",
    "\n",
    "lamda1=[0.9999,0.5,0.39,0.1,0.05,0.02,0.01,0.005]\n",
    "\n",
    "# Compute λ values\n",
    "lambda_values = [1 / (-50 / np.log(1 - l)) for l in lamda1]\n",
    "\n",
    "# Compute P[IM_i] using the given formula\n",
    "P_IM = []\n",
    "for t in range(1, len(lambda_values) - 1):  # Avoid first and last index\n",
    "    P_IM_i = (lambda_values[t - 1] - lambda_values[t + 1]) / 2\n",
    "    P_IM.append(P_IM_i)\n",
    "\n",
    "\n",
    "# Create a dictionary mapping return periods to P[IM_i]\n",
    "P_IM_dict = {data['1/years'][i]: P_IM[i] for i in range(len(P_IM))}\n",
    "\n",
    "# Multiply each \"CDF4-{r}\" column by the corresponding P[IM_i]\n",
    "for r in data['1/years']:  # Exclude first and last periods as they lack P[IM_i]\n",
    "    column_name = f\"CombDS4-{r}\"\n",
    "    new_column_name = f\"{column_name}*P[IM_i]\"\n",
    "\n",
    "    if column_name in DI_combined_P1.columns:  \n",
    "        DI_combined_P1[new_column_name] = DI_combined_P1[column_name] * P_IM_dict[r]\n",
    "\n",
    "# Add a new column that sums all the new columns\n",
    "new_columns = [f\"CombDS4-{r}*P[IM_i]\" for r in data['1/years'] if f\"CombDS4-{r}\" in DI_combined_P1.columns]\n",
    "\n",
    "# Create a new column \"Sum_of_new_columns\" by summing across the new columns\n",
    "DI_combined_P1[\"Annual Collapse Probability\"] = DI_combined_P1[new_columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e916f1-565f-4094-8cc6-2060c9cee92f",
   "metadata": {},
   "source": [
    "Create and save the final DataFrame containing the damage index (loss ratio) for different return periods along with the annual probability of collapse for each exposure point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b6e10-2663-47c4-9fe7-a354a79e6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate df and IM horizontally, aligning by their index\n",
    "common_df_combined_P1 = pd.concat([df, IM, ky, PGD, Landcdf, DI_combined_P1], axis=1)\n",
    "\n",
    "# Define the path first\n",
    "seimsic_landslide_P1_file_path = savepath / f\"Multi-Hazard/Earthquakes & Landslides/DI_Earthquakes_&_Landslides_P1_ESRM20_{asset}.csv\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "seimsic_landslide_P1_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "common_df_combined_P1.to_csv(seimsic_landslide_P1_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c103aede-5edf-4bb3-81b4-88dff9e14712",
   "metadata": {},
   "source": [
    "Visualise loss ratio map for a return period of 1000 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f177f5-d45d-46da-a85d-1c570eb1f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_df_combined_P1['geometry'] = common_df_combined_P1.apply(lambda row: Point(row['Longitude'], row['Latitude']), axis=1)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "combined_P1_gdf = gpd.GeoDataFrame(common_df_combined_P1, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "# Create bins for the DI-0.001 column\n",
    "bins = [0, 0.1, 0.25, 0.4, 1]  # Define the bin ranges\n",
    "labels = ['0-0.1', '0.1-0.25', '0.25-0.4', '0.4-1']  # Labels for the bins\n",
    "\n",
    "# Assign bin values based on 'DI-0.001'\n",
    "combined_P1_gdf['DI_binned'] = pd.cut(combined_P1_gdf['DI-0.001'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "\n",
    "# Interactive map with 4 bins and custom colors\n",
    "combined_P1_gdf.explore(\n",
    "    column=\"DI_binned\",  # Use the binned DI-0.001 column for coloring\n",
    "    cmap='OrRd',         # Optional colormap, we are using custom colors\n",
    "    stroke=True,         # Add stroke (outline) around markers\n",
    "    line_color=\"black\",  # Set the stroke color to black\n",
    "    legend=True,         # Show the legend\n",
    "    legend_kwds={\n",
    "        'caption': 'Loss ratio (T=1000 years)',  # Legend caption\n",
    "        'orientation': 'horizontal'             # Horizontal legend\n",
    "    },\n",
    "    marker_kwds={\n",
    "        'radius': 5,   # Set marker size (adjust as needed)\n",
    "        'fill': True,   # Fill the markers with color\n",
    "    },\n",
    "    tooltip=\"DI-0.001\",  # Show the DI-0.001 value in the tooltip\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01edb353-4bc8-4615-957e-60cc577b2dd6",
   "metadata": {},
   "source": [
    "### PL < 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f57dd3-95a3-42b1-bcb3-8c1b5b2f1034",
   "metadata": {},
   "source": [
    "Calculate the damage index (loss ratio) assuming that the probability of an earthquake-induced landslide is equal to the probability of the earthquake itself (i.e., $P_L = 1.0$), meaning the landslide will certainly occur given that the earthquake occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61554a1e-9c79-4329-ad52-eccfd4073b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relevant columns to numeric if they are not already\n",
    "DI_seismic = DI_seismic.apply(pd.to_numeric, errors='coerce')\n",
    "Landcdf = Landcdf.apply(pd.to_numeric, errors='coerce')\n",
    "Plandslide_df = Plandslide_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Extract years_values from the Scenaria DataFrame\n",
    "years_values = Scenaria['1/years'].tolist()  # Use the annual probabilities as years values\n",
    "\n",
    "\n",
    "# Initialize a new DataFrame for combined results\n",
    "DI_combined = pd.DataFrame({\n",
    "})\n",
    "\n",
    "# List of dataset names (DS1, DS2, DS3, DS4)\n",
    "datasets = [1,2,3,4]\n",
    "\n",
    "# Loop through each years_value and perform the calculations for each dataset\n",
    "for years_value in years_values:\n",
    "    for dataset in datasets:\n",
    "        combined_value = (\n",
    "            DI_seismic[f\"CDF{dataset}-{years_value}\"] + \n",
    "            Plandslide_df[f'Plandslide-{years_value}'] * Landcdf[f\"LandDS{dataset}-{years_value}\"] -\n",
    "            DI_seismic[f\"CDF{dataset}-{years_value}\"] * Plandslide_df[f'Plandslide-{years_value}'] * Landcdf[f\"LandDS{dataset}-{years_value}\"]\n",
    "        )\n",
    "        \n",
    "        # Store the results in the new DataFrame\n",
    "        DI_combined[f\"CombDS{dataset}-{years_value}\"] = combined_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c4310e-f705-440e-ba32-07b1635b1760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each years_value and perform the calculations for each dataset\n",
    "for years_value in years_values:\n",
    "\n",
    "    # Calculate probabilities of occurrence for each damage state\n",
    "    DI_combined[f\"P_DS1-{years_value}\"] = DI_combined[f\"CombDS1-{years_value}\"] - DI_combined[f\"CombDS2-{years_value}\"]\n",
    "    DI_combined[f\"P_DS2-{years_value}\"] = DI_combined[f\"CombDS2-{years_value}\"] - DI_combined[f\"CombDS3-{years_value}\"]\n",
    "    DI_combined[f\"P_DS3-{years_value}\"] = DI_combined[f\"CombDS3-{years_value}\"] - DI_combined[f\"CombDS4-{years_value}\"]\n",
    "    DI_combined[f\"P_DS4-{years_value}\"] = DI_combined[f\"CombDS4-{years_value}\"]\n",
    "\n",
    "    # Calculate damage index for the scenario based on Material\n",
    "    # This assumes you want to check the Material for each row\n",
    "    for index in range(len(DI_combined)):\n",
    "        if df['Material'].iloc[index] == 'Precast':\n",
    "            DI_combined.at[index, f\"DI-{years_value}\"] = (\n",
    "                DI_combined.at[index, f\"P_DS1-{years_value}\"] * 0.05 +\n",
    "                DI_combined.at[index, f\"P_DS2-{years_value}\"] * 0.30 +\n",
    "                DI_combined.at[index, f\"P_DS3-{years_value}\"] * 0.70 +\n",
    "                DI_combined.at[index, f\"P_DS4-{years_value}\"] * 1\n",
    "            )\n",
    "        else:\n",
    "            DI_combined.at[index, f\"DI-{years_value}\"] = (\n",
    "                DI_combined.at[index, f\"P_DS1-{years_value}\"] * 0.05 +\n",
    "                DI_combined.at[index, f\"P_DS2-{years_value}\"] * 0.15 +\n",
    "                DI_combined.at[index, f\"P_DS3-{years_value}\"] * 0.60 +\n",
    "                DI_combined.at[index, f\"P_DS4-{years_value}\"] * 1\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26254cdb-2afd-4e9d-ad0a-04de1af6a844",
   "metadata": {},
   "source": [
    "Calculate annual collapse probability based on strest methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44534d2d-5bde-458e-8349-ccaeb76d9373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define return periods\n",
    "Periods = [5,73,102,475,1000,2500,5000,10000]\n",
    "\n",
    "lamda1=[0.9999,0.5,0.39,0.1,0.05,0.02,0.01,0.005]\n",
    "\n",
    "# Compute λ values\n",
    "lambda_values = [1 / (-50 / np.log(1 - l)) for l in lamda1]\n",
    "\n",
    "# Compute P[IM_i] using the given formula\n",
    "P_IM = []\n",
    "for t in range(1, len(lambda_values) - 1):  # Avoid first and last index\n",
    "    P_IM_i = (lambda_values[t - 1] - lambda_values[t + 1]) / 2\n",
    "    P_IM.append(P_IM_i)\n",
    "\n",
    "\n",
    "# Create a dictionary mapping return periods to P[IM_i]\n",
    "P_IM_dict = {data['1/years'][i]: P_IM[i] for i in range(len(P_IM))}\n",
    "\n",
    "# Multiply each \"CDF4-{r}\" column by the corresponding P[IM_i]\n",
    "for r in data['1/years']:  # Exclude first and last periods as they lack P[IM_i]\n",
    "    column_name = f\"CombDS4-{r}\"\n",
    "    new_column_name = f\"{column_name}*P[IM_i]\"\n",
    "\n",
    "    if column_name in DI_combined.columns:  \n",
    "        DI_combined[new_column_name] = DI_combined[column_name] * P_IM_dict[r]\n",
    "\n",
    "# Add a new column that sums all the new columns\n",
    "new_columns = [f\"CombDS4-{r}*P[IM_i]\" for r in data['1/years'] if f\"CombDS4-{r}\" in DI_combined.columns]\n",
    "\n",
    "# Create a new column \"Sum_of_new_columns\" by summing across the new columns\n",
    "DI_combined[\"Annual Collapse Probability\"] = DI_combined[new_columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c737f-d66e-454f-8582-de14ef250261",
   "metadata": {},
   "source": [
    "Create and save the final DataFrame containing the damage index (loss ratio) for different return periods along with the annual probability of collapse for each exposure point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f817f0-15fb-4c7b-95ff-ec3a5d4c7ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate df and IM horizontally, aligning by their index\n",
    "common_df_combined = pd.concat([df, IM, ky, PGD, Landcdf, DI_combined], axis=1)\n",
    "\n",
    "# Define the path first\n",
    "seimsic_landslide_file_path = savepath / f\"Multi-Hazard/Earthquakes & Landslides/DI_Earthquakes_&_Landslides_Plandslide_ESRM20_{asset}.csv\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "seimsic_landslide_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "common_df_combined.to_csv(seimsic_landslide_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567f6350-8dc6-4ace-b92c-6457969d9cce",
   "metadata": {},
   "source": [
    "Visualise loss ratio map for a return period of 1000 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b00865d-46a6-4c82-889c-ba2c92bd823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_df_combined['geometry'] = common_df_combined.apply(lambda row: Point(row['Longitude'], row['Latitude']), axis=1)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "combined_gdf = gpd.GeoDataFrame(common_df_combined, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "# Create bins for the DI-0.001 column\n",
    "bins = [0, 0.1, 0.25, 0.4, 1]  # Define the bin ranges\n",
    "labels = ['0-0.1', '0.1-0.25', '0.25-0.4', '0.4-1']  # Labels for the bins\n",
    "\n",
    "# Assign bin values based on 'DI-0.001'\n",
    "combined_gdf['DI_binned'] = pd.cut(combined_gdf['DI-0.001'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "\n",
    "# Interactive map with 4 bins and custom colors\n",
    "combined_gdf.explore(\n",
    "    column=\"DI_binned\",  # Use the binned DI-0.001 column for coloring\n",
    "    cmap='OrRd',         # Optional colormap, we are using custom colors\n",
    "    stroke=True,         # Add stroke (outline) around markers\n",
    "    line_color=\"black\",  # Set the stroke color to black\n",
    "    legend=True,         # Show the legend\n",
    "    legend_kwds={\n",
    "        'caption': 'Loss ratio (T=1000 years)',  # Legend caption\n",
    "        'orientation': 'horizontal'             # Horizontal legend\n",
    "    },\n",
    "    marker_kwds={\n",
    "        'radius': 5,   # Set marker size (adjust as needed)\n",
    "        'fill': True,   # Fill the markers with color\n",
    "    },\n",
    "    tooltip=\"DI-0.001\",  # Show the DI-0.001 value in the tooltip\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miraca",
   "language": "python",
   "name": "miraca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
